<!DOCTYPE html>
<html>

<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    
    <title>ahljljj</title>
    <meta charset="utf-8">
    
    <meta name="title" content="ahljljj">
    <meta name="description" content="">
    <meta property="og:image" content="/favicon.png">
    <meta property="og:image:width" content="200" />
    <meta property="og:image:height" content="200" />
    <link rel="shortcut icon" href="/favicon.png">
    
    <link rel="stylesheet"
        href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/arduino-light.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    
    
<link rel="stylesheet" href="/css/style.css">

    
<script src="/js/jquery.min.js"></script>

    
<script src="/js/saki.js"></script>

<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header">
  <div class="blog-title">
    <a href="/" class="logo">ahljljj</a>
  </div>
  <nav class="navbar">
    <ul class="menu">
      
      
      
      <li class="menu-item">
        
        <a href="/" class="current-menu-item-link">主页</a>
        
      </li>
      
      
      
      <li class="menu-item">
        
        <a href="/archives" class="menu-item-link">归档</a>
        
      </li>
      
      
      
      <li class="menu-item">
        
        <a href="/about" class="menu-item-link">关于</a>
        
      </li>
      
      
      
      <li class="menu-item">
        
        <a href="/rss" class="menu-item-link">订阅</a>
        
      </li>
      
    </ul>
  </nav>
</header>
  <main class="main">
    <section class="posts">
  
  <article class="post">
    <div class="post-title">
      <a class="post-title-link" href="/2020/03/20/hello-world/">Hello World</a>
    </div>
    <div class="post-content">
      
      <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    <div class="post-meta">
      <span class="post-time">2020/03/20</span>
    </div>
  </article>
  
  <article class="post">
    <div class="post-title">
      <a class="post-title-link" href="/2018/05/18/2018-05-19-Monotonicity-of-K-means/">Monotonicity of K-means</a>
    </div>
    <div class="post-content">
      
      <h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>Given a set $S={x_1,\cdots,x_n}$ ,  we aim to (1)  partition the $n$ observations into $k(k\leq n)$ subsets $\{s_1,\cdots,s_k\}$, (2) minimize within-cluster sum of squares. Formally, the objective is to find:</p>
<script type="math/tex; mode=display">
\begin{equation}\label{eqn:obj}\underset{s}{\text{argmin}}  \sum_{i=1}^k\sum_{x\in s_i} ||x-c_i||^2\end{equation}</script><p>where $c_i$ is the mean (center of mass) of points in the subset $s_i$.  Now in order to achieve this goal, the K-means algorithm reads as follows, first, we randomly pick $k$ points $\{c_1,\cdots,c_k\}$ which are called centroids and label those points in $S$ that closer to $c_i$ than any other centroids by $c_i$.  Mathematically, we just performed a Voronoi partition on $S$  for the lattice $\{c_1,\cdots,c_k\}$. Next, we want to keep doing this Voronoi partition by updating our lattice so that \ref{eqn:obj} is monotonously decreasing. There are two steps here:</p>
<ul>
<li><p>Update the centroid $c_i$ of each cluster by its center of mass, i.e.,  </p>
<script type="math/tex; mode=display">
\begin{equation}\label{eqn: centroid}c_i=\frac{1}{|s_i|}\sum\_{x\in s_i} x;\end{equation}</script></li>
<li><p>Update the cluster labels by doing Voronoi partition on the new centroids set $\{c_i,\cdots,c_k \}$.</p>
</li>
</ul>
<h2 id="Monotonicity"><a href="#Monotonicity" class="headerlink" title="Monotonicity"></a>Monotonicity</h2><p>The monotonicity of K-means algorithm essentially follows from the basic fact that any function attains the extreme points at the points where its derivative is vanishing. In fact, we consider the following energy:</p>
<script type="math/tex; mode=display">
\begin{equation}\label{eqn:energy}f(c_1,\cdots,c_k)=\sum\_{i=1}^k\sum\_{x\in s_i} ||x-c_i||^2.\end{equation}</script><p>The gradient of $f$ with respect to $c_i$ is</p>
<script type="math/tex; mode=display">
\begin{equation}\label{eqn:grad}\frac{\partial f}{\partial c_i}=-2\sum\_{x\in s_i}(x-c_i),\quad i=1,\cdots k.\end{equation}</script><p>By pushing (\ref{eqn:grad}) $=0$ , it’s clear that </p>
<script type="math/tex; mode=display">
c_i=\frac{1}{|s_i|}\sum_{x\in s_i} x,</script><p>which is exactly the centroid we chose in the above updating step. This also implies that K-means is actually a gradient descent algorithm. Unlike the gradient descent we have seen in many other context, here, we have a closed and simple form for the gradient. So we don’t have to implement the algorithm to obtain the gradient but simply use this elegant math formulation (\ref{eqn: centroid}).</p>

      
    </div>
    <div class="post-meta">
      <span class="post-time">2018/05/18</span>
    </div>
  </article>
  
  <article class="post">
    <div class="post-title">
      <a class="post-title-link" href="/2018/05/15/2018-05-16-Stochastic-gradient-decent-and-gradient-decent/">Stochastic gradient decent and gradient decent</a>
    </div>
    <div class="post-content">
      
      <h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>This analysis explores the relation between the gradient decent and stochastic gradient decent. Starting from the basic gradient decent method, we shall see that SGD actually decrease this energy on “average”.</p>
<h5 id="Gradient-decent-a-math-approach"><a href="#Gradient-decent-a-math-approach" class="headerlink" title="Gradient decent: a math approach"></a>Gradient decent: a math approach</h5><p>Let’s introduce the convex function first.</p>
<ul>
<li>A function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is convex if and only if for all $x,y$ $\in \mathbb{R}^n$ and $\lambda\in (0,1)$, we have<script type="math/tex; mode=display">
f(\lambda x+(1-\lambda)y)\leq \lambda f(x)+(1-\lambda)f(y).</script></li>
</ul>
<p>A more mathematical definition is</p>
<ul>
<li>A function is said to be convex if and only if it’s second derivative is always non-negative.</li>
</ul>
<p>A simple example is the parabola equation $x^2$, and as you could see from the graph, this function has a unique minimum. The reason that we restricted our attention on the convex function is because its minimum could be easily calculated. In theory, to get the minimum/maximum of a function, we could simply set the derivative of it to zero and solve the equation(s).  However, there are two potential issues here.</p>
<ul>
<li>There may be no closed form for the solutions especially when we have a complicated model.</li>
<li>Even there is a explicit formula for the extreme points, the time complexity is $O(n^3)$ , which is another thing we need worry about if we have a super big data set.</li>
</ul>
<p>The gradient decent in machine learning is to find the minimum/maximum of a convex function without calculating the inverse matrix of the derivative. There are many discussions about this context online, here I just take a few lines to talk about the basic idea from the point view of mathematics.</p>
<p>Given a convex function ^[The solution may not be converge to the global minimum without convexity.]  $f:\mathbb{R}^n\rightarrow\mathbb{R}$, we consider the following ordinary differential equation:  </p>
<script type="math/tex; mode=display">
\begin{equation}\label{eqn:grad}\frac{dx}{dt}=-\nabla f,\quad x\in \mathbb{R}^n\end{equation}.</script><p>We then look at the change of $f$ along the solution to  (\ref{eqn:grad}), and it turn out that $f$ is monotonicity decreasing along the solution, in fact,  the chain rule implies</p>
<script type="math/tex; mode=display">
\begin{equation}\label{eqn:mono}\frac{df}{dt}=\frac{df}{dx}\cdot \frac{dx}{dt}=-||\nabla f||^2.\end{equation}</script><p>Therefore, ideally, $f$ should decreases and decreases until it attains a constant. Then the right hand side of (\ref{eqn:mono}) implies that this constant must be the global minimum. (\ref{eqn:mono}) also implies that $f$ decreases fastest if we move $x$ in the direction of its negative gradient. The word “fastest” here could be interpreted as “most efficient” somehow, and it basically comes from the fact that the product of two vectors $a,b$ maximize/minimize if and only if  $a,b$ are parallel to each other. Another thing should be note here is that <strong>there are many other directions that make</strong> $f$ <strong>decreases, they are just not as efficient as</strong> $\nabla f$ <strong>does</strong>. This is the foundation of  SGD!</p>
<h5 id="Gradient-descent-in-regression"><a href="#Gradient-descent-in-regression" class="headerlink" title="Gradient descent in regression"></a>Gradient descent in regression</h5><p>Regression is not as mysterious as its name suggest.  Mathematically, it’s just an algorithm to solve an optimization problem. Let’s consider the simple case: linear regression. The <em>cost/loss</em> function is defined to be sum square error:</p>
<script type="math/tex; mode=display">
\begin{equation}\label{eqn:linear}f(w)=||x\cdot w-y||^2\end{equation}</script><p>where $x\in M^{N\times n}$ is the feature matrix, $w\in M^{n\times 1}$ is the weight vector, and $y\in M^{N\times 1}$ is the true value. We want to find a best $w$ that minimize $f$ based on the known observations $(x,y)$. In other words, we just need find the minimum of $f$ . To do this, we  simply discretize (\ref{eqn:mono}) as follows:</p>
<script type="math/tex; mode=display">
\frac{f_{n+1}-f_n}{h}=-||\nabla f||_n^2,</script><p>the above expression can be rewritten as a standard iteration form:</p>
<script type="math/tex; mode=display">
\begin{equation}\label{alg:linear}f_{n+1}=f_n-h\cdot||\nabla f||_n^2,\end{equation}</script><p>where $h$ is the time step, and $\nabla f$ can be easily obtained in most regression models, for instance, </p>
<script type="math/tex; mode=display">
\nabla f=-2x^T\cdot(x\cdot w-y)</script><p> where $T$ is representing the transpose. Now if we ignore the numerical instabilities (usually caused by a big step-size $h$),  (\ref{eqn:mono}) implies that $f$ would monotonicity decreasing under (\ref{alg:linear})! </p>
<h4 id="Stochastic-descent-gradient-SGD"><a href="#Stochastic-descent-gradient-SGD" class="headerlink" title="Stochastic descent gradient (SGD)"></a>Stochastic descent gradient (SGD)</h4><p>Like gradient descent, SGD is another numerical algorithm to find the minimum/maximum.</p>
<p>TBD.</p>

      
    </div>
    <div class="post-meta">
      <span class="post-time">2018/05/15</span>
    </div>
  </article>
  
  <article class="post">
    <div class="post-title">
      <a class="post-title-link" href="/2018/05/14/2018-05-15-Adaboost-decision-tree-algorithms/">Adaboost decision tree algorithms</a>
    </div>
    <div class="post-content">
      
      <p><em>This is brief introduction of Adaboosting algorithms after I finished the machine learning course in coursera.</em></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>A foundation question in machine learning is: <strong>can a set of weak classifiers be combined to create a strong classifier?</strong> The answer is yes. Adaboost was the first really successful boosting algorithm developed for binary classification.  More importantly, it is so mathematically beautiful and easy to implement that it would definitely be the starting point to learn boosting.  In this document,  we investigate this algorithm by boosting a set of decision tree algorithms. </p>
<h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><p>Let’s consider the implementation of Adaboosting ensemble on decision tree model. Given a set of classifiers <script type="math/tex">\{f_1,f_2,\cdot, f_k\}</script>,  where $f_i=\pm 1$ for $i=1,\cdots, k$ we want to find an appropriate linear combination of these classifiers  </p>
<script type="math/tex; mode=display">F_k=\text{sign}\sum_{i=1}^kw_i f_i</script><p>such that $F_T$ is better that any $f_t$ . <em>How to find these weights</em>: $w_1,\cdots,w_k$ ? Or even more basically, how to choose these classifiers ${f_1,f_2,\cdot, f_k}$?  </p>
<h5 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1."></a>Step 1.</h5><p>To begin with, we choose the first classifier $f_1$ to be the usual decision tree model.</p>

      
    </div>
    <div class="post-meta">
      <span class="post-time">2018/05/14</span>
    </div>
  </article>
  
</section>

<p class="license">本站文章使用 CC BY-NC-SA 4.0 许可证</p>
  </main><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>